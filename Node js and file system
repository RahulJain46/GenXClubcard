1. To work with the file system on the computer: we need to add a file system module. 
To include the File System module, use the require() method:

var fs = require('fs');
  
Asynchronous methods take the last parameter as the completion function callback and the first parameter of the callback function as error. It is better to use an asynchronous method instead of a synchronous method, as the former never blocks a program during its execution, whereas the second one does.
var fs = require("fs");

// Asynchronous read
fs.readFile('input.txt', function (err, data) {
   if (err) {
      return console.error(err);
   }
   console.log("Asynchronous read: " + data.toString());
});

// Synchronous read
var data = fs.readFileSync('input.txt');
console.log("Synchronous read: " + data.toString());

console.log("Program Ended")

fs.open(path, flags[, mode], callback)
path : This is the string having file name including path.
flags − Flags indicate the behavior of the file to be opened. All possible values have been mentioned below.
mode − It sets the file mode (permission and sticky bits), but only if the file was created. It defaults to 0666, readable and writeable.
callback − This is the callback function which gets two arguments (err, fd).



  
  
 Explore :  
1. chaining of callbacks ? 
2.   


Error-first callbacks are used to pass errors and data as well. You have to pass the error as the first parameter, and it has to be checked to see if something went wrong. Additional arguments are used to pass data.
What are Promises?
What's a stub? Name a use case!
What's a test pyramid? Give an example!
lots of low-level unit tests for models (dependencies are stubbed),
fewer integration tests, where you check how your models interact with each other (dependencies are not stubbed),
less end-to-end tests, where you call your actual endpoints (dependencies are not stubbed).
What's your favorite HTTP framework and why?
When are background/worker processes useful? How can you handle worker tasks?


How can you secure your HTTP cookies against XSS attacks?

XSS occurs when the attacker injects executable JavaScript code into the HTML response.

To mitigate these attacks, you have to set flags on the set-cookie HTTP header:

HttpOnly - this attribute is used to help prevent attacks such as cross-site scripting since it does not allow the cookie to be accessed via JavaScript.
secure - this attribute tells the browser to only send the cookie if the request is being sent over HTTPS.
So it would look something like this: Set-Cookie: sid=<cookie-value>; HttpOnly. If you are using Express, with express-cookie session, it is working by default.

1) What is Node.js?
Node.js is a very powerful JavaScript based platform or framework which is built on Google Chrome's JavaScript V8 Engine.


Basic concept of threats : 
hread is an execution unit which consists of its own program counter, a stack, and a set of registers. Threads are also known as Lightweight processes. Threads are popular way to improve application through parallelism. The CPU switches rapidly back and forth among the threads giving illusion that the threads are running in parallel.

Synchronous :
when the client sends a message to the server it waits until the server has responded before continuing with further processing.
major benefit  :implifies programming and makes your life as a developer easier.
Disadvantage : 
users of the client because they are required to wait for the middle tier to finish processing before they can get on with another task. Another issue that is particularly visible when the application isn't used on the local network is lag.
Asynchronous:
D : The development of applications is more complicated and can require more time to implement the needed functionality.


Javascript : Asysnc -

Code is normally placed on the call stack before being executed.
The call stack is a segment of memory that keeps track of the order
of functions in which they were called.
The events loop, is the process in which the browser queues up tasks
and executes them, one at a time, by putting them on the call stack.
Now how does the browser queue up tasks?
In the beginning of an application, all the JavaScript code is run til
completion and placed in the call stack in order of execution.
The event que is initially empty.
As events occur,
event handlers place new tasks onto the event queue.
Some examples of these events are mouse clicks, keyboard presses,
and timed events.
These tasks waits on the events queue until the call stack is empty.
Once the call stack is empty,
the first task in the queue is put on to the stack.
The subsequent tasks wait until the call stack is empty again,
and the cycle repeats.
This cycle is the events loop
Now how do asynchronous functions fit into the event loop?
When the JavaScript runtime comes across an asynchronous
function in the call stack, it does not process it immediately.
Instead of blocking the call stack until it is finished, it allows
another process to handle processing of the asynchronous function.
When the other process has finished,
it adds a task back on to the event queue.
This task Is usually a callback function, passed
in as one of the arguments, to the original asynchronous function.
You are probably thinking to yourselves,
didn't you just say JavaScript was single-threaded?
Well, the JavaScript runtime is single-threaded, but there are other
processes running in a browser, such as timers, input handlers,
and network request APIs that run in parallel with the event loop.
These parallel processes communicates with the event loop
by putting new tasks on to the event queue.
So why are asynchronous functions important?
They are important because some tasks, such as network request
are slow and users will become upset if their browsers freeze up
because of slow network request blocked other code from running.
Asynchronous requests allow slow tasks to be operated
on a separate thread to stop a task from blocking the browser.
Call backs are used to specify what new tasks are sent back to the event
queue once the other process has finished processing
the original task.
And that is the story of the event loop.


A promise is a container that holds the eventual result of
an asynchronous operation.
Before promises,
callbacks were commonly used to handle asynchronous results.
The problem with callbacks is that if there are too many of them
chained together, the code becomes a pyramid of callbacks that is hard to
read and hard to manage, especially when handling errors.
On the other hand, promises are much easier to chain together and
have a great way of handling errors.

Callback - when an asynchronous operation has a result (a result being either returned data or an error that occurred during the operation), it points to a function that will be executed once that result is ready. This function is what we call a “callback function”.

Promise - A promise is an object that wraps an asynchronous operation and notifies when it’s done. This sounds exactly like callbacks, but the important differences are in the usage of Promises.
A promise is an object that may produce a single value some time in the future: either a resolved value, or a reason that it’s not resolved (e.g., a network error occurred). A promise may be in one of 3 possible states: fulfilled, rejected, or pending. Promise users can attach callbacks to handle the fulfilled value or the reason for rejection.

function getAsyncData(someValue){
    return new Promise(function(resolve, reject){
        getData(someValue, function(error, result){
            if(error){
                reject(error);
            }
            else{
                resolve(result);
            }
        })
    });
}

getAsyncData(“someValue”)
// Calling resolve in the Promise will get us here, to the first then(…)
.then(function(result){
    // Do stuff
})
// Calling reject in the Promise will get us here, to the catch(…)
// Also if there is an error in any then(..) it will end up here
.catch(function(error){
    // Handle error
});


The Fetch API is an interface built into
the browser that allows users to make network requests.
Great, So why should you use it?
Well, it is a much needed improvement over the XML HTTP
request API, which was the previous method for making that work request.
The XML HTTP request API required a lot of boilerplate code,
which made it very hard to follow.
Developers often used third party libraries such as jQuery
to wrap the XML HTTP request API in a much more usable way.
However, with the fetch API,
developers now have an easy to use method for
making network requests that doesn't require any external libraries.
So how easy is it to use the Fetch API?
Well, in its simplest form, all you need to use is the Fetch method and
plug in the URL endpoint of your network request, and that's it.
The Fetch method will return a promise that will hold a fetch
response, and from that response you can obtain the data that you
are trying to receive.



Tree shaking : Webpack 2
So, what we've learned is that in order to take advantage of tree shaking, you must...

Use ES2015 module syntax (i.e. import and export).
Include a minifier that supports dead code removal (e.g. the UglifyJSPlugin).
You can imagine your application as a tree. The source code and libraries you actually use represent the green, living leaves of the tree. Dead code represents the brown, dead leaves of the tree that are consumed by autumn. In order to get rid of the dead leaves, you have to shake the tree, causing them fall.

performace optimisation :
Profiling Components with Chrome Timeline
You can now visualize React components in the Chrome Timeline. This lets you see which components exactly get mounted, updated, and unmounted, how much time they take relative to each other.

Another React performance tool worth mentioning is why-did-you-update. This npm package patches React to emit console warnings whenever a component rerenders with identical props. Caveats: The output is verbose, and it doesn't work on functional components.

What makes a React app slow is, most of the time, useless rerenders of many components.
shouldComponentUpdate(). By default, React always renders a component to the virtual DOM. In other terms, it's your job as a developer to check that the props of a component didn't change and skip rendering altogether in that case.
There are many other things you should do to keep your React app fast (using keys, lazy loading heavy routes, the react-addons-perf package, using ServiceWorkers to cache app state, going isomorphic, etc), but implementing shouldComponentUpdate correctly is the first step - and the most rewarding.


There actually is an acronym for the types of applications Node is designed for: DIRT.
It stands for data-intensive real-time applications. Because Node itself is very lightweight
on I/O, it’s good at shuffling or proxying data from one pipe to another. It allows a
server to hold a number of connections open while handling many requests and keeping
a small memory footprint.

To handle chat-related messaging, you could poll the server with Ajax. But to make
this application as responsive as possible, you’ll avoid using traditional Ajax as a means
to send messages. Ajax uses HTTP as a transport mechanism, and HTTP wasn’t
designed for real-time communication. When a message is sent using HTTP, a new
TCP/IP connection must be used. Opening and closing connections takes time, and
the size of the data transfer is larger because HTTP headers are sent on every request.
Instead of employing a solution reliant on HTTP,

Why Node JS  - 

Easy to heavy CPU I/O tasks.

